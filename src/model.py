{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global index\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "states = [\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA',\n",
    "    'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK',\n",
    "    'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "]\n",
    "\n",
    "\n",
    "def process_data_and_save_to_csv(input_file, output_file):\n",
    "    # import the dataset\n",
    "    data = pd.read_csv(input_file)\n",
    "    # Filter out rows for US territories\n",
    "    data = data[data['state'].isin(states)]\n",
    "\n",
    "    # Remove duplicates based on year and incident_type\n",
    "    data = data.drop_duplicates(subset=['state', 'fy_declared', 'incident_type'])\n",
    "\n",
    "    # create new DataFrame\n",
    "    dataset = pd.DataFrame(data)\n",
    "\n",
    "    # Get the 8 most common incident types\n",
    "    common_incidents = dataset['incident_type'].value_counts().nlargest(5).index.tolist()\n",
    "\n",
    "    # Create new rows for each state, year, and incident type, indicating if it occurred\n",
    "    # Dictionary to store the most common incident types for each state and year\n",
    "    state_year_incidents = {}\n",
    "    # Iterate over each row in the dataset\n",
    "    for index, row in dataset.iterrows():\n",
    "        state = row['state']\n",
    "        year = row['fy_declared']\n",
    "        incident_type = row['incident_type']\n",
    "\n",
    "        # Check if this incident type is one of the 6 most common\n",
    "        if incident_type in common_incidents:\n",
    "            # Check if we already have an entry for this state and year\n",
    "            if (state, year) not in state_year_incidents:\n",
    "                # If not, create a new entry with all incident types set to 0\n",
    "                state_year_incidents[(state, year)] = {incident: 0 for incident in common_incidents}\n",
    "\n",
    "            # Set the is_occured value to 1 for this incident type\n",
    "            state_year_incidents[(state, year)][incident_type] = 1\n",
    "    # Create a new DataFrame from the state_year_incidents dictionary\n",
    "    new_data = []\n",
    "    for (state, year), incidents in state_year_incidents.items():\n",
    "        for incident, is_occured in incidents.items():\n",
    "            new_data.append([state, year, incident, is_occured])\n",
    "\n",
    "    # update dataset to the new format\n",
    "    dataset = pd.DataFrame(new_data, columns=['state', 'fy_declared', 'incident_type', 'is_occured'])\n",
    "\n",
    "    # Save the processed data to a new CSV file\n",
    "    dataset.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# for performance, check if processed file already exists, else run thi\n",
    "import os\n",
    "input_file = 'us_disaster_declarations.csv'\n",
    "output_file = 'processed_data.csv'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    process_data_and_save_to_csv(input_file, output_file)\n",
    "\n",
    "dataset = pd.read_csv(output_file)\n",
    "common_incidents = dataset['incident_type'].unique()\n",
    "\n",
    "# Separate features (state and fy_declared) and target variable (incident_type)\n",
    "X = dataset[['state', 'fy_declared', 'incident_type']]\n",
    "y = dataset['is_occured']\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(sparse_output=False), [0, 2])],\n",
    "                       remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "# splitting the dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "# Training the logistic regression model on the training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='rbf', random_state=42, probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def predict_by_year(input_year):\n",
    "    state_names = {\n",
    "        'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "        'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "        'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "        'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts',\n",
    "        'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana',\n",
    "        'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico',\n",
    "        'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
    "        'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "    }\n",
    "\n",
    "    # Get a JSON list of predictions\n",
    "    # format the numbers to be usable on the client side\n",
    "    predictions = []\n",
    "    for state in states:\n",
    "        state_predictions = {}\n",
    "        state_predictions[\"state\"] = state\n",
    "        state_predictions[\"state_full\"] = state_names[state]  # Add the full state name\n",
    "        state_predictions[\"predictions\"] = {}\n",
    "        avg_pred_total = 0;\n",
    "        for incident in common_incidents:\n",
    "            new_input = pd.DataFrame({'state': [state], 'fy_declared': [input_year], 'incident_type': [incident]})\n",
    "            X_new = np.array(ct.transform(new_input))\n",
    "            X_new_scaled = sc.transform(X_new)\n",
    "            probabilities = classifier.predict_proba(X_new_scaled)\n",
    "            prediction_value = round(probabilities[0][1] * 100, 2)\n",
    "            probability_occurrence = prediction_value\n",
    "            avg_pred_total += prediction_value\n",
    "            state_predictions[\"predictions\"][incident] = probability_occurrence\n",
    "        state_predictions[\"predictions\"][\"Avg\"] = round(avg_pred_total / len(common_incidents), 2)\n",
    "        predictions.append(state_predictions)\n",
    "    # Convert predictions to JSON format\n",
    "    import json\n",
    "    output_json = json.dumps(predictions, indent=2)\n",
    "    return output_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
