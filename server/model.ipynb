{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "global index\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load df with states cleaned\n",
    "df=pd.read_parquet('../data/state_df.parquet')\n",
    "\n",
    "def process_data_and_save_to_csv(df, output_file):\n",
    "    # Remove duplicates based on year (fy_declared) and incident_type\n",
    "    data=df.drop_duplicates(subset=['state', 'fy_declared', 'incident_type'])\n",
    "\n",
    "    # Get 5 most common incident types\n",
    "    common_incidents=data['incident_type'].value_counts().nlargest(5).index.tolist()\n",
    "\n",
    "    # Create new rows for each state, year, and incident type, indicating if it occurred\n",
    "    # Create dictionary to store the most common incident types for each state and year\n",
    "    state_year_incidents = {}\n",
    "\n",
    "    # Iterate over each row in the dataset\n",
    "    for index, row in data.iterrows():\n",
    "        state=row['state']\n",
    "        year=row['fy_declared']\n",
    "        incident_type=row['incident_type']\n",
    "\n",
    "        # Check if this incident type is one of the 5 most common\n",
    "        if incident_type in common_incidents:\n",
    "            # Check if we already have an entry for this state and year\n",
    "            if (state, year) not in state_year_incidents:\n",
    "                # If not, create a new entry with all incident types set to 0\n",
    "                state_year_incidents[(state, year)]={incident: 0 for incident in common_incidents}\n",
    "\n",
    "            # Set the occurred value to 1 for this incident type\n",
    "            state_year_incidents[(state, year)][incident_type] = 1\n",
    "\n",
    "    # Create a new DataFrame from the state_year_incidents dictionary\n",
    "    new_data = []\n",
    "    for (state, year), incidents in state_year_incidents.items():\n",
    "        for incident, occurred in incidents.items():\n",
    "            new_data.append([state, year, incident, occurred])\n",
    "\n",
    "    # Update dataset to the new format\n",
    "    data=pd.DataFrame(new_data, columns=['state', 'fy_declared', 'incident_type', 'occurred'])\n",
    "\n",
    "    # Save the processed data to a new CSV file\n",
    "    data.to_csv(output_file, index=False)\n",
    "\n",
    "# Check if processed file already exists, else run this (for performance)\n",
    "input_file='us_disaster_declarations.csv'\n",
    "output_file='processed_data.csv'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    process_data_and_save_to_csv(df, output_file)\n",
    "\n",
    "data=pd.read_csv(output_file)\n",
    "common_incidents=data['incident_type'].unique()\n",
    "\n",
    "# Separate features (state and fy_declared) and target variable (incident_type)\n",
    "X=data[['state', 'fy_declared', 'incident_type']]\n",
    "y=data['occurred']\n",
    "\n",
    "# Encode categorical data and independent variable\n",
    "ct=ColumnTransformer(transformers=[('encoder', OneHotEncoder(sparse_output=False), [0, 2])],\n",
    "                       remainder='passthrough')\n",
    "X=np.array(ct.fit_transform(X))\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "# Train model on the training set\n",
    "classifier=SVC(kernel='rbf', random_state=42, probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "def predict_by_year(input_year):\n",
    "    states=[\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA',\n",
    "    'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK',\n",
    "    'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "    ]\n",
    "    \n",
    "    state_names={\n",
    "        'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "        'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "        'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "        'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts',\n",
    "        'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana',\n",
    "        'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico',\n",
    "        'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
    "        'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "    }\n",
    "\n",
    "    # Get a JSON list of predictions and format the numbers to be usable on the client side\n",
    "    predictions=[]\n",
    "    for state in states:\n",
    "        state_predictions={}\n",
    "        state_predictions[\"state\"]=state\n",
    "        state_predictions[\"state_full\"]=state_names[state]  # Add the full state name\n",
    "        state_predictions[\"predictions\"]={}\n",
    "        avg_pred_total=0;\n",
    "        for incident in common_incidents:\n",
    "            new_input=pd.DataFrame({'state': [state], 'fy_declared': [input_year], 'incident_type': [incident]})\n",
    "            X_new=np.array(ct.transform(new_input))\n",
    "            X_new_scaled=sc.transform(X_new)\n",
    "            probabilities=classifier.predict_proba(X_new_scaled)\n",
    "            prediction_value=round(probabilities[0][1] * 100, 2)\n",
    "            probability_occurrence=prediction_value\n",
    "            avg_pred_total+=prediction_value\n",
    "            state_predictions[\"predictions\"][incident]=probability_occurrence\n",
    "        state_predictions[\"predictions\"][\"Avg\"]=round(avg_pred_total / len(common_incidents), 2)\n",
    "        predictions.append(state_predictions)\n",
    "    # Convert predictions to JSON format\n",
    "    import json\n",
    "    output_json=json.dumps(predictions, indent=2)\n",
    "    return output_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
