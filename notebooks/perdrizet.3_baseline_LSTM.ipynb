{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM model\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# PyPI imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, roc_curve, precision_recall_curve\n",
    "\n",
    "# Set GPU for TensorFlow\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "\n",
    "# Suppress warning and info messages from TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "# Internal imports\n",
    "import perdrizet_helper_funcs\n",
    "\n",
    "# Input data\n",
    "data_file='../data/resampled_disaster_data_all.parquet'\n",
    "\n",
    "# Incident feature to use\n",
    "incident_feature='linear_log_incidents'\n",
    "\n",
    "# Training details\n",
    "include_synthetic_features=False\n",
    "window=1\n",
    "training_iterations=100\n",
    "training_epochs=5\n",
    "learning_rate=0.0001\n",
    "l1_weight=0.0001\n",
    "l2_weight=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df=pd.read_parquet(data_file)\n",
    "\n",
    "# Grab the features we are interested in\n",
    "data_df=raw_data_df[['month_cos', 'month_sin', 'incidents_binary', incident_feature]].copy()\n",
    "\n",
    "# Transfer the index\n",
    "data_df.set_index(raw_data_df.index)\n",
    "\n",
    "# Save the index of the target column\n",
    "target_column_index=data_df.columns.get_loc(incident_feature)\n",
    "data_df.head()\n",
    "\n",
    "# Save the index of the target column\n",
    "target_column_index=data_df.columns.get_loc('incidents_binary')\n",
    "\n",
    "# Get and show some summary statistics\n",
    "total_disaster_months=len(data_df[data_df['incidents_binary'] != 0])\n",
    "percent_disaster_months=(total_disaster_months/len(data_df)) * 100\n",
    "print(f'Have {total_disaster_months}({percent_disaster_months:.1f}%) disaster months\\n')\n",
    "\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a quick plot to check the distribution of disaster counts\n",
    "plt.title('Incident distribution')\n",
    "plt.hist(data_df[incident_feature], bins=30, color='black')\n",
    "plt.xlabel('Incidents')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "plt.savefig('./figures/3-1-baseline_LSTM_incident_distribution.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-test split\n",
    "\n",
    "Take the most recent ~10% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of years\n",
    "years=data_df.index.get_level_values('year').unique().tolist()\n",
    "testing_years=len(years) // 10\n",
    "print(f'Using most recent {testing_years} of {len(years)} years for test set')\n",
    "print(f'Testing years: {years[-testing_years:]}')\n",
    "print(f'Training years: {years[:-testing_years]}')\n",
    "\n",
    "# Take last n years for testing data\n",
    "testing_df=data_df.loc[years[-testing_years:]]\n",
    "\n",
    "# Take the rest for training\n",
    "training_df=data_df.loc[years[:-testing_years]]\n",
    "\n",
    "# Check result\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data formatting & splitting\n",
    "\n",
    "### 3.1. Training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run custom splitting function\n",
    "training_features, training_labels, validation_features, validation_labels, states=perdrizet_helper_funcs.make_windowed_time_course(training_df, target_column_index, window)\n",
    "\n",
    "# Check the result\n",
    "print(f'State batches: {len(training_features)}')\n",
    "\n",
    "for state, features, labels in zip(states, training_features, training_labels):\n",
    "    print(f'{state}: features: {features.shape}, labels: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format test data for predictions\n",
    "testing_features=[]\n",
    "testing_labels=[]\n",
    "\n",
    "for i in range(len(testing_df) - window - 1):\n",
    "\n",
    "    testing_features.append(testing_df.iloc[i:i + window])\n",
    "    testing_labels.append([testing_df.iloc[i + window + 1,target_column_index]])\n",
    "\n",
    "# Check the result\n",
    "print(f'Testing features shape: {np.array(testing_features).shape}')\n",
    "print(f'Testing labels shape: {np.array(testing_labels).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial model training\n",
    "\n",
    "### 4.1. Set-up and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model=perdrizet_helper_funcs.build_lstm(\n",
    "    training_features[0].shape[1],\n",
    "    training_features[0].shape[2],\n",
    "    learning_rate=learning_rate,\n",
    "    l1_weight=l1_weight,\n",
    "    l2_weight=l2_weight\n",
    ")\n",
    "\n",
    "# Print out the model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Naive model test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make the predictions\n",
    "# predictions=[]\n",
    "\n",
    "# for features in testing_features:\n",
    "#     predictions.extend(model.predict(np.array([features]), verbose=0))\n",
    "\n",
    "# predictions_df=pd.DataFrame.from_dict({'labels': np.array(testing_labels).flatten(), 'probabilities': np.array(predictions).flatten()})\n",
    "# predictions_df.head()\n",
    "\n",
    "# # Set threshold and call incidents\n",
    "# threshold=0.5\n",
    "# calls=np.where(predictions_df['probabilities'] > threshold, 1, 0)\n",
    "\n",
    "# # Calculate precision and recall\n",
    "# precision=precision_score(predictions_df['labels'], calls)\n",
    "# recall=recall_score(testing_labels, calls)\n",
    "# print(f'Precision: {precision:.3f}')\n",
    "# print(f'Recall: {recall:.3f}\\n')\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# cm=confusion_matrix(predictions_df['labels'], calls, normalize='true')\n",
    "# cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no incident', 'incident'])\n",
    "# _=cm_disp.plot()\n",
    "# plt.title('Naive LSTM test set performance')\n",
    "# plt.savefig('./figures/3.4.2-naive_baseline_LSTM_test_set_performance.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get the class weights\n",
    "class_weight=perdrizet_helper_funcs.get_class_weights(training_df)\n",
    "\n",
    "# Manually train the model iteratively over the state time courses using each for a single batch training run\n",
    "true_epoch=0\n",
    "training_results=[]\n",
    "metrics=['loss', 'precision', 'recall', 'val_loss', 'val_precision', 'val_recall']\n",
    "\n",
    "for iteration in range(training_iterations):\n",
    "\n",
    "    # Make an empty dict to store metric results for this iteration\n",
    "    metric_data={}\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_data[metric]=[]\n",
    "\n",
    "    # Loop over the state time courses\n",
    "    for i in range(len(training_features)):\n",
    "\n",
    "        # Train on this state\n",
    "        batch_result, model=perdrizet_helper_funcs.train_lstm(\n",
    "            model,\n",
    "            training_features[i],\n",
    "            training_labels[i],\n",
    "            validation_features[i],\n",
    "            validation_labels[i],\n",
    "            class_weight,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_features[i].shape[0]\n",
    "        )\n",
    "\n",
    "        # Count total training epochs\n",
    "        true_epoch+=training_epochs\n",
    "\n",
    "        # Collect the state-level training result\n",
    "        training_results.append(batch_result)\n",
    "\n",
    "        # Collect metrics for state-level training run\n",
    "        for metric in metrics:\n",
    "            metric_data[metric].extend(batch_result.history[metric])\n",
    "\n",
    "    # Every 10th iteration, save results and print performance summary\n",
    "    if iteration % 10 == 0:\n",
    "\n",
    "        model.save(f'../data/model_checkpoints/baseline_LSTM/epoch_{true_epoch}.keras')\n",
    "\n",
    "        with open(f'../data/training_results/baseline_LSTM.pkl', 'wb') as output_file:\n",
    "            pickle.dump(training_results, output_file)\n",
    "\n",
    "        print(f'Training iteration {iteration} mean: ', end='')\n",
    "\n",
    "        for metric in metrics:\n",
    "            print(f'{metric}={sum(metric_data[metric])/len(metric_data[metric]):.3f} ', end='')\n",
    "        \n",
    "        print()\n",
    "\n",
    "        plot=perdrizet_helper_funcs.plot_single_training_run(\n",
    "            'Baseline LSTM training curves',\n",
    "            training_results,\n",
    "            len(states),\n",
    "            training_epochs\n",
    "        )\n",
    "        plot.savefig('./figures/3-4.3-baseline_LSTM_training_curves.jpg')\n",
    "        plot.close()\n",
    "        \n",
    "# Draw the final plot after the last loop\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model evaluation\n",
    "\n",
    "### 5.1. Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "predictions=[]\n",
    "\n",
    "for features in testing_features:\n",
    "    predictions.extend(model.predict(np.array([features]), verbose=0))\n",
    "\n",
    "predictions_df=pd.DataFrame.from_dict({'labels': np.array(testing_labels).flatten(), 'probabilities': np.array(predictions).flatten()})\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Distribution of predicted incident probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Baseline features LSTM: predicted test set probabilities')\n",
    "plt.hist(predictions_df['probabilities'][predictions_df['labels'] == 0], bins=30, density=True, alpha=0.5, label='No incident')\n",
    "plt.hist(predictions_df['probabilities'][predictions_df['labels'] == 1], bins=30, density=True, alpha=0.5, label='Incident')\n",
    "plt.xlabel('Predicted probability of incident')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('./figures/3-5.2-baseline_LSTM_predicted_probability_distributions.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Receiver-operator characteristic and precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up a 1x2 figure\n",
    "fig, axs=plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('Baseline LSTM test set performance', size='large')\n",
    "\n",
    "# Plot ROC curve\n",
    "fp, tp, _ = roc_curve(predictions_df['labels'], predictions_df['probabilities'])\n",
    "axs[0].set_title('ROC curve')\n",
    "axs[0].plot(100*fp, 100*tp, color='black')\n",
    "axs[0].set_xlabel('False positives [%]')\n",
    "axs[0].set_ylabel('True positives [%]')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_aspect('equal')\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(predictions_df['labels'], predictions_df['probabilities'])\n",
    "axs[1].set_title('Precision-recall curve')\n",
    "axs[1].plot(precision, recall, color='black')\n",
    "axs[1].set_xlabel('Precision')\n",
    "axs[1].set_ylabel('Recall')\n",
    "axs[1].grid(True)\n",
    "axs[1].set_aspect('equal')\n",
    "\n",
    "plt.savefig('./figures/3-5.3-baseline_LSTM_ROC_PR_curves.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold and call incidents\n",
    "threshold=0.5\n",
    "calls=np.where(predictions_df['probabilities'] > threshold, 1, 0)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision=precision_score(predictions_df['labels'], calls)\n",
    "recall=recall_score(testing_labels, calls)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}\\n')\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm=confusion_matrix(predictions_df['labels'], calls, normalize='true')\n",
    "cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no incident', 'incident'])\n",
    "_=cm_disp.plot()\n",
    "plt.title('Baseline LSTM test set performance')\n",
    "plt.savefig('./figures/3-5.4-baseline_LSTM_confusion_matrix.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
