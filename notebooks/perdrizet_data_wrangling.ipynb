{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download disaster data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DataFrame with a specific version of a CSV\n",
    "raw_data_df=kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    'headsortails/us-natural-disaster-declarations',\n",
    "    'us_disaster_declarations.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df.to_parquet('../data/raw_disaster_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download FIPS code database from census.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_df=pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/national_county2020.txt', sep='|')\n",
    "fips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "fips_df.to_parquet('../data/fips_codes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add county name based on FIPS code\n",
    "\n",
    "To add the county name, we need to concatenate the `STATEFP` and `COUNTYFP` columns in the census data, then use that string to translate between the `fips` column in the disaster data and the `COUNTYNAME` column in the census data. Let's put the logic in a function for easy refactoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fips(fips_df: pd.DataFrame, raw_data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Takes census.gov FIPS dataframe and disaster dataframe, adds human readable \n",
    "    county column to disaster data using the census data as look-up table returns \n",
    "    updated disaster dataframe'''\n",
    "\n",
    "    # First extract the state and county FIPS codes\n",
    "    state_fp=fips_df['STATEFP'].to_list()\n",
    "    county_fp=fips_df['COUNTYFP'].to_list()\n",
    "\n",
    "    # Left zero pad state and county FIPS codes to two and three digits respectively\n",
    "    state_fp=[str(n).zfill(2) for n in state_fp]\n",
    "    county_fp=[str(n).zfill(3) for n in county_fp]\n",
    "\n",
    "    # Concatenate the state and county codes to get the full FIPS code\n",
    "    fips=[i+j for i,j in zip(state_fp, county_fp)]\n",
    "\n",
    "    # Make a dictionary to translate FIPS county codes to county names\n",
    "    fips_lookup=dict(zip(fips, fips_df['COUNTYNAME']))\n",
    "\n",
    "    # Add a new column to the raw disaster data containing the FIPS code to be translated\n",
    "    data_df=raw_data_df.copy()\n",
    "    data_df['county_name']=raw_data_df['fips'].apply(str)\n",
    "\n",
    "    # Translate the column values from FIPS to county name, using 'Unknown' for County if we don't\n",
    "    # have the FIPS code in our dict\n",
    "    data_df['county_name']=data_df['county_name'].map(fips_lookup).fillna('Unknown')\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the decoding\n",
    "data_df=decode_fips(fips_df, raw_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at what we have\n",
    "data_df.tail().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "data_df.to_parquet('../data/disaster_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data shape formatting\n",
    "\n",
    "Now the real work begins - we need to set this dataset up for modeling as a multi-label time-series prediction problem. To make our lives a little easier, we will treat each county as an independent sample (this is likely not strictly true, as especially weather based disasters may tend to co-occur geographically). Doing so increases the number of observations and simplifies the input. Each individual time point will be a vector of dummy encoded disasters, including one feature for 'no-disaster'. This will give is n different time series for the n different counties. Care will need to be taken when splitting/sampling and generating batches not to cross between the time series from different counties.\n",
    "\n",
    "Let's dig in!\n",
    "\n",
    "### 5.1. Feature selection/engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the features we are going to work with into a new dataframe\n",
    "working_df=data_df[['incident_begin_date','fips','incident_type']].copy()\n",
    "\n",
    "# Convert 'incident_begin_date' to month and year columns and set as index\n",
    "working_df['incident_begin_date']=pd.to_datetime(working_df['incident_begin_date'])\n",
    "\n",
    "# Remove duplicates\n",
    "working_df=working_df.drop_duplicates(keep='first')\n",
    "\n",
    "# Fix the index\n",
    "working_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Extract month and year from the dataetime series and add them back to the dataframe\n",
    "# working_df['year'] = datetime_series.dt.year\n",
    "# working_df['month'] = datetime_series.dt.month\n",
    "working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_dummies=pd.get_dummies(working_df['incident_type'], dtype=int)\n",
    "disaster_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df=pd.concat([working_df.reset_index(drop=True), disaster_dummies.reset_index(drop=True)], axis=1)\n",
    "working_df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to regularize the time series to a frequency of months across the span of years within each county. As we do so, there will be months with no disaster that will need to be filled in with a 'none' value.\n",
    "\n",
    "## 5.2. Time series regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_months(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Takes a yearly groupby object and sums features over months'''\n",
    "\n",
    "    group=group.resample('ME').sum()\n",
    "\n",
    "    return group\n",
    "\n",
    "def resample_months(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Takes working dataframe and resamples frequency to months.\n",
    "    Returns updated dataframe'''\n",
    "\n",
    "    # Set 'incident_begin_date' as datetime axis\n",
    "    group=group.set_index('incident_begin_date')\n",
    "\n",
    "    print(group.head())\n",
    "\n",
    "    # Sum the disasters in each month by year. This removes duplicates where\n",
    "    # there was more than on disaster in a month.\n",
    "    group=group.groupby(group.index.year, group_keys=False).apply(sum_months)\n",
    "\n",
    "\n",
    "    # Resample to monthly frequency\n",
    "    group=group.resample('D').asfreq()\n",
    "\n",
    "    # Fill missing values 'no_disaster'\n",
    "    group['incident_type']=group['incident_type'].fillna(0)\n",
    "\n",
    "    # Reset the index, preserving the `incident_begin_date`\n",
    "    group.reset_index(inplace=True, drop=False)\n",
    "\n",
    "    print(group.head())\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_working_df=working_df.groupby('fips', group_keys=True).apply(resample_months, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
